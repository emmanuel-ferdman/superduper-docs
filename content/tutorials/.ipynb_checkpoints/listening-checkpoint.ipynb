{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aff62002-f228-4b86-8889-c8ee1bb7fa02",
   "metadata": {},
   "source": [
    "# Listening for new data\n",
    "\n",
    "In , AI models may be configured to listen for newly inserted data.\n",
    "Outputs will be computed over that data and saved back to the data-backend.\n",
    "\n",
    "In this example we show how to configure 2 models to interact when new data is added.\n",
    "\n",
    "1. A featurizing computer vision model (images `->` vectors).\n",
    "1. 2 models evaluating image-2-text similarity to a set of key-words."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa08a8ee-fd51-44c8-bd87-2a2d8ba15434",
   "metadata": {},
   "source": [
    "In this tutorial we use an open-source model \"CLIP\" which we install via `pip` directly from GitHub.\n",
    "You can read more about installing requirements on our docs [here](../get_started/environment)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8322b3ca-d351-4214-9c78-23d0d3b8a6a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting git+https://github.com/openai/CLIP.git\n",
      "  Cloning https://github.com/openai/CLIP.git to /private/var/folders/3h/p6qzszds1c7gtbmt_2qq0tvm0000gn/T/pip-req-build-spx4v54y\n",
      "  Running command git clone --filter=blob:none --quiet https://github.com/openai/CLIP.git /private/var/folders/3h/p6qzszds1c7gtbmt_2qq0tvm0000gn/T/pip-req-build-spx4v54y\n",
      "  Resolved https://github.com/openai/CLIP.git to commit a1d071733d7111c9c014f024669f959182114e33\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: ftfy in /Users/dodo/.pyenv/versions/3.11.7/envs/superduper-3.11/lib/python3.11/site-packages (from clip==1.0) (6.2.0)\n",
      "Requirement already satisfied: regex in /Users/dodo/.pyenv/versions/3.11.7/envs/superduper-3.11/lib/python3.11/site-packages (from clip==1.0) (2023.12.25)\n",
      "Requirement already satisfied: tqdm in /Users/dodo/.pyenv/versions/3.11.7/envs/superduper-3.11/lib/python3.11/site-packages (from clip==1.0) (4.66.2)\n",
      "Requirement already satisfied: torch in /Users/dodo/.pyenv/versions/3.11.7/envs/superduper-3.11/lib/python3.11/site-packages (from clip==1.0) (2.2.2)\n",
      "Requirement already satisfied: torchvision in /Users/dodo/.pyenv/versions/3.11.7/envs/superduper-3.11/lib/python3.11/site-packages (from clip==1.0) (0.17.2)\n",
      "Requirement already satisfied: wcwidth<0.3.0,>=0.2.12 in /Users/dodo/.pyenv/versions/3.11.7/envs/superduper-3.11/lib/python3.11/site-packages (from ftfy->clip==1.0) (0.2.13)\n",
      "Requirement already satisfied: filelock in /Users/dodo/.pyenv/versions/3.11.7/envs/superduper-3.11/lib/python3.11/site-packages (from torch->clip==1.0) (3.13.3)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /Users/dodo/.pyenv/versions/3.11.7/envs/superduper-3.11/lib/python3.11/site-packages (from torch->clip==1.0) (4.11.0)\n",
      "Requirement already satisfied: sympy in /Users/dodo/.pyenv/versions/3.11.7/envs/superduper-3.11/lib/python3.11/site-packages (from torch->clip==1.0) (1.12)\n",
      "Requirement already satisfied: networkx in /Users/dodo/.pyenv/versions/3.11.7/envs/superduper-3.11/lib/python3.11/site-packages (from torch->clip==1.0) (3.3)\n",
      "Requirement already satisfied: jinja2 in /Users/dodo/.pyenv/versions/3.11.7/envs/superduper-3.11/lib/python3.11/site-packages (from torch->clip==1.0) (3.1.3)\n",
      "Requirement already satisfied: fsspec in /Users/dodo/.pyenv/versions/3.11.7/envs/superduper-3.11/lib/python3.11/site-packages (from torch->clip==1.0) (2024.2.0)\n",
      "Requirement already satisfied: numpy in /Users/dodo/.pyenv/versions/3.11.7/envs/superduper-3.11/lib/python3.11/site-packages (from torchvision->clip==1.0) (1.26.4)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /Users/dodo/.pyenv/versions/3.11.7/envs/superduper-3.11/lib/python3.11/site-packages (from torchvision->clip==1.0) (10.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/dodo/.pyenv/versions/3.11.7/envs/superduper-3.11/lib/python3.11/site-packages (from jinja2->torch->clip==1.0) (2.1.5)\n",
      "Requirement already satisfied: mpmath>=0.19 in /Users/dodo/.pyenv/versions/3.11.7/envs/superduper-3.11/lib/python3.11/site-packages (from sympy->torch->clip==1.0) (1.3.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.2.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.0\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install git+https://github.com/openai/CLIP.git"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53c6934d-8bb9-4922-b196-12592b89f037",
   "metadata": {},
   "source": [
    "In this tutorial we show case how to \"connect\" models to the data in the database and how to connect\n",
    "additional models to the output of those initial models. \n",
    "\n",
    "To demonstrate that this works on any type of data, we apply this setup to images from the \n",
    "[cats and dogs dataset](https://www.kaggle.com/c/dogs-vs-cats). We've prepared a subset especially \n",
    "for quick experimentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "49e546ab-3433-44a3-8de0-7ee7a8960541",
   "metadata": {},
   "outputs": [],
   "source": [
    "!curl -O https://superduper-public-demo.s3.amazonaws.com/images.zip && unzip images.zip\n",
    "from PIL import Image\n",
    "import os\n",
    "\n",
    "data = [f'images/{x}' for x in os.listdir('./images') if x.endswith('png')]\n",
    "data = [{'img': Image.open(path)} for path in data]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da2f1db7-1370-4c65-8c15-065411d6b214",
   "metadata": {},
   "source": [
    "Now that we've prepared these records we can insert this data \"directly\" into the database with \n",
    "a standard MongoDB insert statement. (Notice however the difference from `pymongo` with the `.execute()` call.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "065924be-8d12-40e6-a930-2f1bf6b2e28d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from superduper import superduper, Document\n",
    "\n",
    "db = superduper('mongomock://')\n",
    "\n",
    "db['images'].insert_many([Document(r) for r in data[:-1]]).execute()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7df57ddc-4574-49f8-ba17-c53a8024888d",
   "metadata": {},
   "source": [
    "We can verify that the images are correctly saved by retrieved a single record:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31c405b9-58ee-4af6-9b1f-c52d583739e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "r = db['images'].find_one().execute()\n",
    "r"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fe359f9-5609-4ebd-b145-e8301a7f1136",
   "metadata": {},
   "source": [
    "The contents of the `Document` may be accessed by calling `.unpack()`. You can see that the images were saved and retrieved correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ac30c30-cf45-4ec3-85e6-185c5939d3ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "r.unpack()['img']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10ea86f3-8094-4dd4-bdcc-81dd49716865",
   "metadata": {},
   "source": [
    "We now build a `torch` model for text-2-image similarity using the `clip` library. In order to \n",
    "save the outputs correctly in the system, we add the `tensor` datatype to the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ef3d07b-b396-479f-bce1-568232dcfd86",
   "metadata": {},
   "outputs": [],
   "source": [
    "import clip\n",
    "import torch\n",
    "from superduper.ext.torch import TorchModel, tensor\n",
    "\n",
    "\n",
    "model, preprocess = clip.load(\"ViT-B/32\", \"cpu\")\n",
    "\n",
    "class ImageModel(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "\n",
    "    def forward(self, image_tensors):\n",
    "        return self.model.encode_image(image_tensors)\n",
    "\n",
    "\n",
    "dt = tensor(dtype='float', shape=(512,))\n",
    "\n",
    "\n",
    "image_model = TorchModel(\n",
    "    identifier='clip_image',\n",
    "    object=ImageModel(),\n",
    "    preprocess=preprocess,\n",
    "    datatype=dt,\n",
    "    loader_kwargs={'batch_size': 5},\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb47ca72-1b13-4c79-aec0-3499eefb1d0f",
   "metadata": {},
   "source": [
    "We can verify that this model gives us the correct outputs on the added data with the `.predict_one` method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99157957-cec2-4873-9d18-615a6822053a",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_model.predict_one(data[0]['img'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a072cb67-bc43-4973-9b9d-aae8067ef041",
   "metadata": {},
   "source": [
    "Now we'd like to set up this model to compute outputs on the `'img'` key of each record. \n",
    "To do that we create a `Listener` (see [here](../apply_api/listener) for more information) which \n",
    "\"listens\" for incoming and existing data, and computes outputs on that data.\n",
    "\n",
    "When new data is inserted, the model automatically will create outputs on that data. This is a very handy \n",
    "feature for productionizing AI and ML, since a data deployment needs to be keep up-to-date as far as possible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b56e896a-5c4f-4943-bc6b-9a36ef8c52b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "listener = image_model.to_listener(\n",
    "    select=db['images'].find(),\n",
    "    key='img',\n",
    "    identifier='image_predictions',\n",
    ")\n",
    "\n",
    "_ = db.apply(listener)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca856a29-4c2f-45bb-acc8-2f21e9a10005",
   "metadata": {},
   "source": [
    "We can verify that the outputs are correctly inserted into the documents with this query. \n",
    "The outputs are saved in the `listener.outputs` field:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1927b211-d7a8-42f7-9f2a-13fa93c4b12e",
   "metadata": {},
   "outputs": [],
   "source": [
    "list(listener.outputs_select.limit(1).execute())[0][listener.outputs].unpack()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0950fff9-f86e-4197-96e6-b0cc61470522",
   "metadata": {},
   "source": [
    "Downstream of this first model, we now can add another smaller model, to classify images with configurable terms. \n",
    "Since the dataset is concerned with cats and dogs we create 2 downstream models classifying the images in 2 different ways."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5a553db-47ba-4bdf-9644-e63cc21c405c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from superduper import ObjectModel\n",
    "\n",
    "\n",
    "class Comparer:\n",
    "    def __init__(self, words, text_features):\n",
    "        self.targets = {w: text_features[i] for i, w in enumerate(words)}\n",
    "        self.lookup = list(self.targets.keys())\n",
    "        self.matrix = torch.stack(list(self.targets.values()))\n",
    "\n",
    "    def __call__(self, vector):\n",
    "        best = (self.matrix @ vector).topk(1)[1].item()\n",
    "        return self.lookup[best]\n",
    "\n",
    "\n",
    "cats_vs_dogs = ObjectModel(\n",
    "    'cats_vs_dogs',\n",
    "    object=Comparer(['cat', 'dog'], model.encode_text(clip.tokenize(['cat', 'dog']))),\n",
    ").to_listener(\n",
    "    select=db['images'].find(),\n",
    "    key=listener.outputs,\n",
    ")\n",
    "\n",
    "            \n",
    "felines_vs_canines = ObjectModel(\n",
    "    'felines_vs_canines',\n",
    "    object=Comparer(['feline', 'canine'], model.encode_text(clip.tokenize(['feline', 'canine']))),\n",
    ").to_listener(\n",
    "    select=db['images'].find(),\n",
    "    key=listener.outputs,\n",
    ")\n",
    "\n",
    "\n",
    "db.apply(cats_vs_dogs)\n",
    "db.apply(felines_vs_canines)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b4a4ade-d2bb-4196-9b58-8f7ff732b694",
   "metadata": {},
   "source": [
    "We can verify that both downstream models have written their outputs to the database by querying a document:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc068d95-a889-48e6-81e6-89f931463c86",
   "metadata": {},
   "outputs": [],
   "source": [
    "r = db['images'].find_one().execute()\n",
    "\n",
    "print(r[cats_vs_dogs.outputs])\n",
    "print(r[felines_vs_canines.outputs])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02f1f22c-2683-4f2e-a860-083dfdd1c424",
   "metadata": {},
   "source": [
    "Let's check that the predictions make sense for the inserted images:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34f95f98-d439-457b-af45-092b1523c6c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "db['images'].find_one({cats_vs_dogs.outputs: 'cat'}).execute()['img']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75b957ef-9430-45bd-ad04-301db24565e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "db['images'].find_one({cats_vs_dogs.outputs: 'feline'}).execute()['img']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a10f0e84-abc3-4e23-9a2f-6f54e2f71534",
   "metadata": {},
   "outputs": [],
   "source": [
    "db['images'].find_one({cats_vs_dogs.outputs: 'dog'}).execute()['img']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "093436e2-d382-4a87-8c60-d47aceb8a555",
   "metadata": {},
   "outputs": [],
   "source": [
    "db['images'].find_one({cats_vs_dogs.outputs: 'canine'}).execute()['img']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c981b3f5-3142-4cf2-a5a6-4a134e0a01ac",
   "metadata": {},
   "source": [
    "Now that we have installed the models using `Listener`, we can insert new data. This \n",
    "data should be automatically processed by the installed models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fcf1b10-cd18-4a99-99cd-b94a71350bab",
   "metadata": {},
   "outputs": [],
   "source": [
    "db['images'].insert_one(Document({**data[-1], 'new': True})).execute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14ccd0fa-620a-4b10-bc02-72ee1a4a6361",
   "metadata": {},
   "outputs": [],
   "source": [
    "We can verify this by querying the data again:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da19733f-aee1-432f-8aee-6b5074a212d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "r = db['images'].find_one({'new': True}).execute().unpack()\n",
    "r['img']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea6ba071-cde3-4cc1-89c4-00c3ce88151b",
   "metadata": {},
   "source": [
    "You see here that the models have been called in the correct order on the newly added data and the outputs saved \n",
    "to the new record:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c783504-14c9-413f-a6ea-9c2ea83aa944",
   "metadata": {},
   "outputs": [],
   "source": [
    "r['_outputs']"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
